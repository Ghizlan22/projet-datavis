import pandas as pd
from io import StringIO
import os
import re
from sklearn.impute import KNNImputer
import numpy as np

# === SECTION MANQUANTE : DOCUMENTATION DU JEU DE DONN√âES ===
"""
CONTEXTE DES DONN√âES (√† ajouter en en-t√™te):
- Dataset recyclage: Taux de recyclage des d√©chets municipaux par pays
- Dataset d√©chets: Quantit√© totale de d√©chets agricoles par pays
- P√©riode: Donn√©es historiques jusqu'en 2024
- Source: Open data environnemental (√† pr√©ciser)
"""
def validate_data_structure(df, dataset_name):
    """Valide la structure des donn√©es pour l'analyse"""
    print(f"\n=== VALIDATION STRUCTURE {dataset_name} ===")
    print(f"Shape: {df.shape}")
    print(f"Colonnes: {df.columns.tolist()}")
    print(f"Types:\n{df.dtypes}")
    if len(df) > 0:
        print(f"P√©riode couverte: {df['Year'].min()} - {df['Year'].max()}")
    else:
        print("P√©riode couverte: DataFrame vide")
    return True


# === AJOUT: V√âRIFICATION DE LA COH√âRENCE TEMPORELLE ===
def check_temporal_consistency(df, year_col='Year'):
    """V√©rifie la continuit√© temporelle des donn√©es"""
    if len(df) == 0:
        print("‚úÖ DataFrame vide - aucune v√©rification temporelle n√©cessaire")
        return []
    
    years = sorted(df[year_col].unique())
    gaps = []
    for i in range(1, len(years)):
        if years[i] - years[i-1] > 1:
            gaps.append((years[i-1], years[i]))
    
    if gaps:
        print(f"‚ö†Ô∏è  Gaps temporels d√©tect√©s: {gaps}")
    else:
        print("‚úÖ Continuit√© temporelle v√©rifi√©e")
    return gaps

# === AJOUT: ANALYSE DES VALEURS EXTR√äMES ===
def detect_outliers_iqr(df, column):
    """D√©tecte les valeurs aberrantes avec la m√©thode IQR"""
    if len(df) == 0:
        print(f"Valeurs aberrantes dans {column}: 0 (DataFrame vide)")
        return pd.DataFrame()
    
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    print(f"Valeurs aberrantes dans {column}: {len(outliers)}")
    return outliers


orig_path = r"C:\Users\USER\Desktop\Data vis project\data2.csv"
clean_path = r"C:\Users\USER\Desktop\Data vis project\data2_cleaned.csv"

# 1) Nettoyer le fichier ligne par ligne : enlever " en d√©but et fin de ligne s'ils existent
with open(orig_path, "r", encoding="utf-8") as f_in, open(clean_path, "w", encoding="utf-8", newline="\n") as f_out:
    for line in f_in:
        # supprime \r\n -> \n, strip seulement des guillemets extr√™mes
        line = line.rstrip("\r\n")
        if len(line) >= 2 and line[0] == '"' and line[-1] == '"':
            line = line[1:-1]  # enl√®ve les guillemets ext√©rieurs
        f_out.write(line + "\n")

print("Fichier nettoy√© √©crit dans :", clean_path)

# 2) Lire le fichier nettoy√© avec pandas
df2 = pd.read_csv(clean_path, sep=",", engine="python")

print("Colonnes d√©tect√©es dans data2_cleaned.csv :", df2.columns.tolist())
print(df2.head())

# === Nettoyage des donn√©es recyclage ===

# Renommer les colonnes pour plus de clart√©
df2 = df2.rename(columns={
    "Entity": "Country",
    "Code": "Code",
    "Year": "Year",
    "Variable:% Recycling - MUNW": "RecyclingRate"
})

# NE PLUS SUPPRIMER les lignes avec des valeurs manquantes
# df2 = df2.dropna(subset=["Country", "Year", "RecyclingRate"])

# Supprimer les doublons (si le m√™me pays + ann√©e appara√Æt plusieurs fois)
df2 = df2.drop_duplicates(subset=["Country", "Year"])

# Convertir les types de donn√©es
df2["Year"] = df2["Year"].astype(int)
df2["RecyclingRate"] = df2["RecyclingRate"].astype(float)

# V√©rifier les valeurs extr√™mes (pour rep√©rer erreurs √©ventuelles)
print("Taux min / max :", df2["RecyclingRate"].min(), "/", df2["RecyclingRate"].max())

# === V√âRIFICATION ET IMPUTATION DES VALEURS MANQUANTES - DATASET RECYCLAGE ===

print("\n" + "="*50)
print("V√âRIFICATION DES VALEURS MANQUANTES - RECYCLAGE")
print("="*50)

print("Valeurs manquantes avant imputation:")
print(df2.isnull().sum())

# V√©rifier s'il y a des valeurs manquantes dans RecyclingRate
if df2['RecyclingRate'].isnull().sum() > 0:
    print(f"\nIl y a {df2['RecyclingRate'].isnull().sum()} valeurs manquantes dans RecyclingRate")
    print("Application de l'imputation KNN...")
    
    # Pr√©parer les donn√©es pour KNN
    df2_numeric = df2[['Year', 'RecyclingRate']].copy()
    
    # Appliquer KNN Imputer
    imputer = KNNImputer(n_neighbors=5)
    df2_imputed = imputer.fit_transform(df2_numeric)
    
    # Remplacer les valeurs dans le DataFrame original
    df2['RecyclingRate'] = df2_imputed[:, 1]
    
    print("Imputation KNN termin√©e!")
else:
    print("Aucune valeur manquante dans RecyclingRate - pas besoin d'imputation")

print("\nValeurs manquantes apr√®s imputation:")
print(df2.isnull().sum())

# Enregistrer ce dataset propre
df2.to_csv("C:\\Users\\USER\\Desktop\\Data vis project\\data\\recycling_clean.csv", index=False)

# === Nettoyage du fichier data3 ===

orig_path3 = r"C:\Users\USER\Desktop\Data vis project\data3.csv"
clean_path3 = r"C:\Users\USER\Desktop\Data vis project\data3_cleaned.csv"

# 1Ô∏è‚É£ Lecture + nettoyage g√©n√©ral (supprimer guillemets et ; en fin de ligne)
cleaned_lines = []
with open(orig_path3, "r", encoding="utf-8", errors="ignore") as f_in:
    for line in f_in:
        line = line.strip()
        line = line.replace('""', '"').replace(';"', '"').replace(';', ',')
        if line.startswith('"') and line.endswith('"'):
            line = line[1:-1]
        cleaned_lines.append(line)

# 2Ô∏è‚É£ Recomposer le texte en un seul bloc et forcer les vraies nouvelles lignes
raw_text = "\n".join(cleaned_lines)
# On divise les lignes quand on voit un motif de d√©but de pays (majuscule suivie de virgule)
rows = re.findall(r'([A-Z][^A-Z]+)', raw_text)

# 3Ô∏è‚É£ Construire la table
data = []
for row in rows:
    parts = [p.strip() for p in row.split(",") if p.strip() != ""]
    if len(parts) >= 4:
        data.append(parts[:10])  # on garde les 10 premi√®res colonnes environ

# 4Ô∏è‚É£ D√©terminer les colonnes (si elles existent dans la premi√®re ligne)
columns = ["Entity", "Code", "Year", "Agriculture", "Households", "Construction",
           "Manufacturing", "Electricity", "Mining", "Other_services"]

df3 = pd.DataFrame(data, columns=columns)

# 5Ô∏è‚É£ Garder seulement les colonnes principales
df3_simple = df3[["Entity", "Code", "Year", "Agriculture"]].copy()
df3_simple = df3_simple.rename(columns={"Agriculture": "TotalWaste"})

# NE PLUS SUPPRIMER les lignes avec des valeurs manquantes
# df3_simple = df3_simple.dropna(subset=["Entity", "Year", "TotalWaste"])

# Garder uniquement les ann√©es valides (ex : 1990‚Äì2030)
df3_simple = df3_simple[df3_simple["Year"].astype(str).str.match(r"^\d{4}$")]

# Convertir les types
df3_simple["Year"] = df3_simple["Year"].astype(int)

# === DIAGNOSTIC URGENT - ANALYSE DATA3 AVANT FILTRAGE ===
print("\n" + "="*60)
print("DIAGNOSTIC COMPLET DU FICHIER DATA3")
print("="*60)

# Afficher toutes les ann√©es uniques AVANT filtrage
print("Ann√©es uniques dans data3 AVANT filtrage:")
annees_uniques = sorted(df3_simple["Year"].unique())
print(annees_uniques[:20])  # Afficher les 20 premi√®res pour √©viter overflow

# Afficher un √©chantillon des donn√©es brutes
print("\n√âchantillon des donn√©es AVANT filtrage:")
print(df3_simple.head(20))

# V√©rifier la plage r√©elle des ann√©es
print(f"\nPlage des ann√©es: {df3_simple['Year'].min()} √† {df3_simple['Year'].max()}")
print(f"Nombre total de lignes AVANT filtrage: {len(df3_simple)}")

# === CORRECTION INTELLIGENTE - FILTRAGE ANN√âES D√âCHETS ===
annees_avant = len(df3_simple)

# Analyser la distribution des ann√©es pour choisir le bon filtre
annee_min = df3_simple['Year'].min()
annee_max = df3_simple['Year'].max()

print(f"\nüîç ANALYSE: Ann√©es de {annee_min} √† {annee_max}")

if annee_min < 1900 or annee_max > 2050:
    print("üö® DONN√âES ANORMALES: Les ann√©es sont en dehors des plages r√©alistes!")
    print("üìã STRAT√âGIE: On garde TOUTES les donn√©es pour analyse manuelle")
    # On ne filtre pas - on garde tout pour voir le probl√®me
    df3_simple_filtered = df3_simple.copy()
else:
    # Filtrer normalement
    df3_simple_filtered = df3_simple[(df3_simple["Year"] >= 1990) & (df3_simple["Year"] <= 2030)]

df3_simple = df3_simple_filtered
annees_apres = len(df3_simple)

print(f"üîß CORRECTION ANN√âES: {annees_avant} ‚Üí {annees_apres} lignes apr√®s traitement")
if len(df3_simple) > 0:
    print(f"Plage temporelle corrig√©e: {df3_simple['Year'].min()} - {df3_simple['Year'].max()}")
else:
    print("‚ö†Ô∏è  ATTENTION: Plus de donn√©es apr√®s traitement!")

# Nettoyer la colonne TotalWaste (seulement si on a des donn√©es)
if len(df3_simple) > 0:
    df3_simple["TotalWaste"] = (
        df3_simple["TotalWaste"]
        .astype(str)
        .str.replace(",", "")
        .str.extract(r"(\d+\.?\d*)")[0]  # extraire les nombres
        .astype(float)
    )

    # Supprimer seulement les valeurs n√©gatives, pas les NaN
    df3_simple = df3_simple[(df3_simple["TotalWaste"] > 0) | (df3_simple["TotalWaste"].isna())]

# === V√âRIFICATION ET IMPUTATION DES VALEURS MANQUANTES - DATASET D√âCHETS ===

print("\n" + "="*50)
print("V√âRIFICATION DES VALEURS MANQUANTES - D√âCHETS")
print("="*50)

if len(df3_simple) == 0:
    print("‚ö†Ô∏è  Dataset d√©chets VIDE - aucune v√©rification n√©cessaire")
else:
    print("Valeurs manquantes avant imputation:")
    print(df3_simple.isnull().sum())

    # V√©rifier s'il y a des valeurs manquantes dans TotalWaste
    if df3_simple['TotalWaste'].isnull().sum() > 0:
        print(f"\nIl y a {df3_simple['TotalWaste'].isnull().sum()} valeurs manquantes dans TotalWaste")
        print("Application de l'imputation KNN...")
        
        # Pr√©parer les donn√©es pour KNN
        df3_numeric = df3_simple[['Year', 'TotalWaste']].copy()
        
        # Appliquer KNN Imputer
        imputer = KNNImputer(n_neighbors=5)
        df3_imputed = imputer.fit_transform(df3_numeric)
        
        # Remplacer les valeurs dans le DataFrame original
        df3_simple['TotalWaste'] = df3_imputed[:, 1]
        
        print("Imputation KNN termin√©e!")
    else:
        print("Aucune valeur manquante dans TotalWaste - pas besoin d'imputation")

    print("\nValeurs manquantes apr√®s imputation:")
    print(df3_simple.isnull().sum())

# Sauvegarde (m√™me si vide, pour √©viter les erreurs)
df3_simple.to_csv("C:\\Users\\USER\\Desktop\\Data vis project\\data\\waste_clean.csv", index=False)
print("‚úÖ Donn√©es d√©chets enregistr√©es dans data/waste_clean.csv")

if len(df3_simple) > 0:
    print("Aper√ßu des donn√©es d√©chets:")
    print(df3_simple.head(10))
else:
    print("‚ö†Ô∏è  Fichier d√©chets sauvegard√© mais VIDE")

# VALIDATION DES DONN√âES TRAIT√âES
print("\n" + "="*60)
print("VALIDATION FINALE DES DONN√âES PR√âPAR√âES")
print("="*60)

# Validation dataset recyclage
validate_data_structure(df2, "RECYCLAGE")
check_temporal_consistency(df2)
detect_outliers_iqr(df2, "RecyclingRate")

# Validation dataset d√©chets  
validate_data_structure(df3_simple, "D√âCHETS")
check_temporal_consistency(df3_simple)
detect_outliers_iqr(df3_simple, "TotalWaste")

# === RAPPORT DE PR√âPARATION COMPLET ===
print("\n" + "="*60)
print("RAPPORT COMPLET PHASE 1 - PR√âPARATION DES DONN√âES")
print("="*60)

print("‚úÖ COLLECTE: Donn√©es environnementales charg√©es depuis CSV")
print("‚úÖ NETTOYAGE: Guillemets, s√©parateurs, encodage corrig√©s")
print("‚úÖ VALEURS MANQUANTES: Trait√©es avec KNN Imputer")
print("‚úÖ DOUBLONS: Supprim√©s avec drop_duplicates()")
print("‚úÖ HOMOG√âN√âISATION: Types de donn√©es standardis√©s")
print("‚úÖ STRUCTURATION: Colonnes Year/Country/Indicateurs organis√©es")
print("üîç POINTS DE VIGILANCE: V√©rifier les gaps temporels et valeurs extr√™mes")

print(f"\nüìä DATASETS PR√äTS POUR L'ANALYSE:")
print(f"   - Recycling: {df2.shape} lignes, {len(df2['Country'].unique())} pays")
print(f"   - D√©chets: {df3_simple.shape} lignes, {len(df3_simple['Entity'].unique()) if len(df3_simple) > 0 else 0} entit√©s")

# === RECOMMANDATION FINALE ===
if len(df3_simple) == 0:
    print("\nüö® RECOMMANDATION URGENTE:")
    print("   Le dataset d√©chets est VIDE apr√®s traitement.")
    print("   V√©rifiez le fichier data3.csv - il contient probablement des donn√©es corrompues ou mal format√©es.")
    print("   Solution: Trouvez un autre fichier de donn√©es pour le th√®me d√©chets.")